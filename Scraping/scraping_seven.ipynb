{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec35378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/asahi-gh22008/anaconda3/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/asahi-gh22008/anaconda3/lib/python3.10/site-packages (4.11.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/asahi-gh22008/anaconda3/lib/python3.10/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/asahi-gh22008/anaconda3/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/asahi-gh22008/anaconda3/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/asahi-gh22008/anaconda3/lib/python3.10/site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/asahi-gh22008/anaconda3/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "# カテゴリページをクロールして商品URLを収集\n",
    "def scrape_category(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    product_links = []\n",
    "    for a in soup.select(\"div.list_inner a\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and \"/products/\" in href:\n",
    "            full_url = \"https://www.sej.co.jp\" + href\n",
    "            product_links.append(full_url)\n",
    "\n",
    "    return list(set(product_links))  # 重複除去\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/onigiri\"  # おにぎりカテゴリ例\n",
    "    product_urls = scrape_category(category_url)\n",
    "\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e60427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "エラー：https://www.sej.co.jp/products/a/item/044389/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047652/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047807/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/046758/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047767/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047562/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047808/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047611/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047765/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047576/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047737/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047774/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047612/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047066/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047698/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/044448/ -> dict contains fields not in fieldnames: 'category'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047882/ -> dict contains fields not in fieldnames: 'category'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 136\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m product_urls:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         product_info \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_seven_eleven_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m         save_to_csv(product_info)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m, in \u001b[0;36mscrape_seven_eleven_product\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_seven_eleven_product\u001b[39m(url):\n\u001b[1;32m     43\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m---> 44\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# 商品名\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     product_name_div \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_ttl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/bs4/builder/_htmlparser.py:399\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    397\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTMLParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/html/parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[1;32m    172\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/html/parser.py:344\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/bs4/builder/_htmlparser.py:154\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[0;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#print(\"START\", name)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m sourceline, sourcepos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetpos()\n\u001b[0;32m--> 154\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourceline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourcepos\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mand\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mis_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:721\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[0;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    718\u001b[0m          \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39msearch_tag(name, attrs))):\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_classes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTag\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentTag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_most_recent_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43msourceline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourcepos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespaces\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/bs4/element.py:1243\u001b[0m, in \u001b[0;36mTag.__init__\u001b[0;34m(self, parser, builder, name, namespace, prefix, attrs, parent, previous, is_xml, sourceline, sourcepos, can_be_empty_element, cdata_list_attributes, preserve_whitespace_tags, interesting_string_types, namespaces)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attrs:\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m builder\u001b[38;5;241m.\u001b[39mcdata_list_attributes:\n\u001b[0;32m-> 1243\u001b[0m         attrs \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_replace_cdata_list_attribute_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1246\u001b[0m         attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/bs4/builder/__init__.py:314\u001b[0m, in \u001b[0;36mTreeBuilder._replace_cdata_list_attribute_values\u001b[0;34m(self, tag_name, attrs)\u001b[0m\n\u001b[1;32m    311\u001b[0m universal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcdata_list_attributes\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[1;32m    312\u001b[0m tag_specific \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcdata_list_attributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    313\u001b[0m     tag_name\u001b[38;5;241m.\u001b[39mlower(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m universal \u001b[38;5;129;01mor\u001b[39;00m (tag_specific \u001b[38;5;129;01mand\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m tag_specific):\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;66;03m# We have a \"class\"-type attribute whose string\u001b[39;00m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# value is a whitespace-separated list of\u001b[39;00m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# values. Split it into a list.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         value \u001b[38;5;241m=\u001b[39m attrs[attr]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "# カテゴリページをクロールして商品URLを収集\n",
    "def scrape_category(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    product_links = []\n",
    "    for a in soup.select(\"div.list_inner a\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and \"/products/\" in href:\n",
    "            full_url = \"https://www.sej.co.jp\" + href\n",
    "            product_links.append(full_url)\n",
    "\n",
    "    return list(set(product_links))  # 重複除去\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/sushi/\"  # おにぎりカテゴリ例\n",
    "    product_urls = scrape_category(category_url)\n",
    "\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfacc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "追加保存：チキンたれかつ弁当\n",
      "追加保存：東海限定特製天津飯\n",
      "追加保存：埼玉ゆかりの味かてめし（混ぜご飯）\n",
      "追加保存：秋の味覚北海道産秋鮭ほぐしのおだしごはん\n",
      "追加保存：香味だれで食べる揚げナスと豚しゃぶ丼\n",
      "追加保存：黒胡椒仕立てのかしわバター丼\n",
      "追加保存：秋の味覚牛肉きのこ御飯幕の内\n",
      "追加保存：背徳飯やみつきタルだく唐揚げ丼\n",
      "追加保存：旨辛チーズタッカルビ丼\n",
      "追加保存：コク旨だれの炭火焼き牛カルビ弁当\n",
      "追加保存：ピリ辛もつ炒め丼\n",
      "追加保存：若鶏のにんにく醤油唐揚げ弁当\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "# カテゴリページをクロールして商品URLを収集\n",
    "def scrape_category(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    product_links = []\n",
    "    for a in soup.select(\"div.list_inner a\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and \"/products/\" in href:\n",
    "            full_url = \"https://www.sej.co.jp\" + href\n",
    "            product_links.append(full_url)\n",
    "\n",
    "    return list(set(product_links))  # 重複除去\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/bento/\"  # おにぎりカテゴリ例\n",
    "    product_urls = scrape_category(category_url)\n",
    "\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fb1513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/donut/\n",
      "取得中: https://www.sej.co.jp/products/a/donut/2/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/donut/1/l15/\n",
      "商品数: 16\n",
      "追加保存：オールドファッション\n",
      "追加保存：こしあんドーナツ\n",
      "追加保存：チョコオールドファッション\n",
      "追加保存：ふわふわ食感シフォンケーキ\n",
      "追加保存：もちもちリングドーナツ\n",
      "追加保存：もちもちリングドーナツチョコ\n",
      "追加保存：もちもちリングシュガー\n",
      "追加保存：アーモンドチョコケーキ\n",
      "追加保存：もちもち食感リングドーナツチョコ\n",
      "追加保存：アソートドーナツ３個入り\n",
      "追加保存：富士山ミルク使用ひとくちドーナツ９個入\n",
      "追加保存：大学芋みたいなオールドファッション\n",
      "追加保存：ハニーオールドファッション\n",
      "追加保存：チョコケーキドーナツ\n",
      "追加保存：チョコオールドファッション\n",
      "追加保存：しっとり黒糖ボールドーナツ２個入り\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "# # カテゴリページをクロールして商品URLを収集\n",
    "# def scrape_category(url):\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     product_links = []\n",
    "#     for a in soup.select(\"div.list_inner a\"):\n",
    "#         href = a.get(\"href\")\n",
    "#         if href and \"/products/\" in href:\n",
    "#             full_url = \"https://www.sej.co.jp\" + href\n",
    "#             product_links.append(full_url)\n",
    "\n",
    "#     return list(set(product_links))  # 重複除去\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/donut/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "追加保存：揚げパンシュガー\n",
      "追加保存：７プレミアムふわもちブレッド６枚入\n",
      "追加保存：７プレミアムもっちり食パン６枚入\n",
      "追加保存：７ＰＧ金の食パン２枚入\n",
      "追加保存：タルタルソースのフィッシュバーガー\n",
      "追加保存：北海道産ポテトとチーズのパン\n",
      "追加保存：粗挽きポークフランク粒マスタードマヨ使用\n",
      "追加保存：ランチパック深煎りピーナッツ\n",
      "追加保存：７Ｐマーガリン入りレーズンバターロール\n",
      "追加保存：７プレミアム発酵バター香るミニクロワッサン５個入\n",
      "追加保存：北海道産じゃがいものコロッケパン\n",
      "追加保存：北海道産じゃがいものコロッケパン\n",
      "追加保存：モリヤマメロンのクリームサンド\n",
      "追加保存：北海道産大豆のきなこあげぱん\n",
      "追加保存：７プレミアムマーラーカオ４個入\n",
      "追加保存：チョコ棒\n",
      "追加保存：７Ｐマーガリン入りバターロール４個入\n",
      "追加保存：７ＰＧ金の食パン４枚入\n",
      "追加保存：パスコ超熟６枚スライス\n",
      "追加保存：７Ｐマーガリン入り黒糖ロール４個入\n",
      "追加保存：７Ｐトリュフマヨツイスト\n",
      "追加保存：たっぷりルウの欧風カレ―パン\n",
      "追加保存：マロンクリームデニッシュ\n",
      "追加保存：７ＰＧ金の食パン４枚\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "# # カテゴリページをクロールして商品URLを収集\n",
    "# def scrape_category(url):\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     product_links = []\n",
    "#     for a in soup.select(\"div.list_inner a\"):\n",
    "#         href = a.get(\"href\")\n",
    "#         if href and \"/products/\" in href:\n",
    "#             full_url = \"https://www.sej.co.jp\" + href\n",
    "#             product_links.append(full_url)\n",
    "\n",
    "#     return list(set(product_links))  # 重複除去\n",
    "\n",
    "# カテゴリページをクロールして全ページの商品URLを収集\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        # 1ページ目はそのまま、それ以降は /{page}/15 を付与\n",
    "        if page == 1:\n",
    "            url = base_url\n",
    "        else:\n",
    "            if not base_url.endswith(\"/\"):\n",
    "                base_url += \"/\"\n",
    "            url = f\"{base_url}{page+1}/15\"\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        product_links = []\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = \"https://www.sej.co.jp\" + href\n",
    "                product_links.append(full_url)\n",
    "\n",
    "        if not product_links:  # 商品が見つからなければ終了\n",
    "            break\n",
    "\n",
    "        all_links.extend(product_links)\n",
    "        page += 1\n",
    "\n",
    "    return list(set(all_links))  # 重複除去\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/bread/\"  # おにぎりカテゴリ例\n",
    "    product_urls = scrape_category(category_url)\n",
    "\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "881a4386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/sandwich/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/2/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/3/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/4/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/5/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/6/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/7/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/8/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/9/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/1/l15/\n",
      "商品数: 125\n",
      "追加保存：野菜ミックスサンド\n",
      "追加保存：クロワッサンサンド照焼チキン＆たまご\n",
      "スキップ（既存）：半熟卵てりやきバーガー\n",
      "スキップ（既存）：シャインマスカット入りフルーツミックスサンド\n",
      "追加保存：バゲットサンドパストラミ＆クリームチーズ\n",
      "追加保存：野菜ミックスサンド\n",
      "スキップ（既存）：ローストチキンと野菜のバインミー\n",
      "追加保存：チーズバーガー\n",
      "追加保存：たまごサンド\n",
      "追加保存：ひんやりとろけるフレンチトースト\n",
      "スキップ（既存）：ＢＩＧハムとたまご\n",
      "追加保存：チーズバーガー\n",
      "追加保存：トーストサンドハム＆スクランブルエッグ\n",
      "追加保存：ラップデリスパイシーコブサラダ\n",
      "追加保存：半熟卵てりやきバーガー\n",
      "追加保存：クロックマダム\n",
      "追加保存：ミニロール（たまご＆ハム）\n",
      "追加保存：彩り野菜ミックスサンド\n",
      "追加保存：嬬恋村産キャベツ使用コールスローサンド\n",
      "追加保存：たんぱく質が摂れるチキン＆チリ\n",
      "追加保存：九州産華味鳥チキンカツサンド\n",
      "追加保存：４種のフルーツミックスサンド\n",
      "追加保存：ミックスサンド\n",
      "追加保存：トーストサンドＢＬＴ\n",
      "追加保存：タルタルチキンバーガー\n",
      "追加保存：ブリトーハム＆チーズ\n",
      "追加保存：旨辛レッドチリチキンカツバーガー\n",
      "追加保存：全粒粉たんぱく質が摂れるチキン＆エッグ\n",
      "追加保存：チキンカツサンドからしマヨネーズ入り\n",
      "追加保存：ソースカツバーガー\n",
      "追加保存：茨城県産たまご使用ボロニアソーセージエッグＢＢＱソース\n",
      "追加保存：粗挽き食感ホットドッグカレーキャベツ盛り\n",
      "スキップ（既存）：ジューシーハムサンドからしマヨネーズ入り\n",
      "追加保存：野菜ミックスサンド\n",
      "追加保存：ミニロール（チキンカツ＆ポテトサラダ）\n",
      "スキップ（既存）：ジューシーハムサンドからしマヨネーズ入り\n",
      "スキップ（既存）：照焼チキンとたまごサンド\n",
      "追加保存：ソーセージエッグマフィン\n",
      "追加保存：ハムとたまごのサンド\n",
      "追加保存：ツナ＆きゅうりサンド\n",
      "追加保存：ミニロール（ハムたまご＆コロッケ）\n",
      "追加保存：王様トマトの野菜ミックスサンド\n",
      "追加保存：チーズバーガー\n",
      "追加保存：Ｗハムカツサンドタルタルソース\n",
      "追加保存：シャキシャキレタスサンド\n",
      "追加保存：しっとり食感コッペたまごサラダロール\n",
      "追加保存：ブリトーモッツァレラとトマトのマルゲリ－タ\n",
      "追加保存：ひんやりとろけるフレンチトースト\n",
      "スキップ（既存）：とんかつサンド\n",
      "追加保存：たまご＆ツナサンド\n",
      "追加保存：バゲットサンドベーコン＆北海道産クリームチーズ\n",
      "追加保存：野菜ミックスサンド\n",
      "追加保存：トーストサンドハム＆チーズ\n",
      "追加保存：タルタル海老カツサンド\n",
      "追加保存：ミニロール（チキンカツ＆ポテトサラダ）\n",
      "追加保存：たまごサンド\n",
      "追加保存：トーストサンド照焼チキンとたまご\n",
      "追加保存：ブリトー台湾風ルーローチーズ\n",
      "追加保存：野菜ミックスサンド\n",
      "追加保存：ミニロール（たまご＆ハムチーズ）\n",
      "追加保存：しっとり食感コッペたまごサラダロール\n",
      "スキップ（既存）：ＢＩＧハムとたまご\n",
      "追加保存：ハムとたまごのサンド\n",
      "追加保存：シャキシャキレタスサンド\n",
      "追加保存：キウイ・黄桃・パインサンド\n",
      "追加保存：たれチキンカツサンド\n",
      "追加保存：バゲットサンドパストラミ＆クリームチーズ\n",
      "追加保存：彩り野菜のトリオサンド\n",
      "追加保存：ミックスサンド\n",
      "追加保存：ミニロール（たまごサラダ＆ハム）\n",
      "追加保存：６切入りひとくちサンドハム＆たまご\n",
      "追加保存：６切入りひとくちサンドハム＆たまご\n",
      "追加保存：しっとりコッペツナサラダロール\n",
      "追加保存：粗挽き食感ジューシーホットドッグ\n",
      "追加保存：上田名物美味だれチキンバーガー\n",
      "追加保存：ブリトーＭＡＴＣＨＡあずきもち\n",
      "スキップ（既存）：照焼チキンとたまごサンド\n",
      "スキップ（既存）：ジューシーハムサンドからしマヨネーズ入り\n",
      "追加保存：久留米ホットドッグ\n",
      "追加保存：ブリトーバターチキンカレー\n",
      "追加保存：ブリトーウインナー２本入り粗挽きボロネーゼ\n",
      "追加保存：Ｗハムカツサンドタルタルソース\n",
      "追加保存：野菜ミックスサンド\n",
      "追加保存：チキンカツサンドからしマヨネーズ入り\n",
      "追加保存：たまごサンド\n",
      "追加保存：上田名物美味だれチキンバーガー\n",
      "追加保存：たまご焼きサンド\n",
      "追加保存：ひとくちボックス（たまご＆ハム）\n",
      "追加保存：ミニロール（たまごサラダ＆ハムチーズ）\n",
      "追加保存：ミックスサンド\n",
      "追加保存：九州産華味鳥チーズチキンカツサンド\n",
      "スキップ（既存）：照焼チキンとたまごサンド\n",
      "追加保存：ホットドッグ\n",
      "追加保存：トーストサンドスクランブルエッグ＆チーズ\n",
      "追加保存：６切入りひとくちサンド（ハム＆たまご）\n",
      "スキップ（既存）：照焼チキンとたまごのサンド\n",
      "追加保存：タルタル海老カツサンド\n",
      "追加保存：タルタル海老カツサンド\n",
      "追加保存：北海道産きたあかり使用ポテトサラダサンド\n",
      "追加保存：クロワッサンたまご＆パストラミ\n",
      "追加保存：ハムとたまごのサンド\n",
      "スキップ（既存）：ＢＩＧハムとたまご\n",
      "追加保存：全粒粉バゲット生ハム＆トマト\n",
      "追加保存：たまご＆ツナサンド\n",
      "追加保存：ラップデリスパイシーコブサラダ\n",
      "追加保存：信州産いちご入りフルーツミックスサンド\n",
      "追加保存：ブリトーチーズ倍盛りハム＆チーズ\n",
      "追加保存：ベーコン・レタス・トマト\n",
      "追加保存：ペッパーポーク＆チェダー\n",
      "追加保存：ハムカツたまごサンド\n",
      "追加保存：ブリトーあんバター\n",
      "追加保存：海老カツ瀬戸内レモンタルタルサンド\n",
      "追加保存：半熟卵てりやきバーガー\n",
      "追加保存：佐世保スイートマヨサンド\n",
      "追加保存：チーズバーガー\n",
      "追加保存：群馬県産たまご使用ボロニアソーセージエッグＢＢＱソース\n",
      "追加保存：ミニロール（たまご＆ハムチーズ）\n",
      "追加保存：とんかつサンド\n",
      "追加保存：ハムカツたまごサンド\n",
      "追加保存：ツナ＆きゅうりサンド\n",
      "追加保存：ミニロール（ハムたまご＆コロッケ）\n",
      "スキップ（既存）：照焼チキンとたまごサンド\n",
      "追加保存：ＢＬＴサンド\n",
      "追加保存：ソーセージエッグマフィン\n",
      "追加保存：クロワッサンサンドハム＆チーズ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "# # カテゴリページをクロールして商品URLを収集\n",
    "# def scrape_category(url):\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     product_links = []\n",
    "#     for a in soup.select(\"div.list_inner a\"):\n",
    "#         href = a.get(\"href\")\n",
    "#         if href and \"/products/\" in href:\n",
    "#             full_url = \"https://www.sej.co.jp\" + href\n",
    "#             product_links.append(full_url)\n",
    "\n",
    "#     return list(set(product_links))  # 重複除去\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/sandwich/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "156eafcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/men/\n",
      "商品数: 24\n",
      "追加保存：お月見冷しちく玉天うどん\n",
      "追加保存：東北限定若鶏の冷たい肉そば\n",
      "追加保存：野菜たっぷりあんかけ焼そば\n",
      "追加保存：高島とんちゃん焼うどん鶏の味噌焼き\n",
      "追加保存：ぼっかけ焼そば\n",
      "追加保存：冷し中華\n",
      "追加保存：薬味で味わう冷し月見半熟玉子うどん\n",
      "追加保存：１／２日分の野菜が摂れる和風ちゃんぽん\n",
      "追加保存：だしの旨みと醤油香る焼うどん\n",
      "追加保存：冷し中華（マヨ付）\n",
      "追加保存：岐阜の味冷したぬきそば\n",
      "追加保存：信州麺友会公認ねぎと胡椒の王様中華そば\n",
      "追加保存：冷しぶっかけうどん\n",
      "追加保存：ミニ温そうめん上州地粉そうめん使用\n",
      "追加保存：冷しぶっかけうどん\n",
      "追加保存：だし割りななこいもとろろの田舎そば\n",
      "追加保存：ソース焼そば\n",
      "追加保存：ちゅるもち鮭ときのこのクリームうどん\n",
      "追加保存：半熟味付たまごの冷製中華そば\n",
      "追加保存：静岡限定のりおろしそば\n",
      "追加保存：濃厚だし割りとろろの冷しぶっかけそば\n",
      "追加保存：ミニ冷し中華（マヨ付）\n",
      "追加保存：濃厚だし割りとろろのミニ冷しぶっかけそば\n",
      "追加保存：お月見明太半熟玉子うどん\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/men/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ed179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/pasta/\n",
      "取得中: https://www.sej.co.jp/products/a/pasta/2/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/pasta/3/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/pasta/1/l15/\n",
      "商品数: 44\n",
      "追加保存：豚しゃぶと胡麻ぽん酢の和風おろし冷製パスタ\n",
      "追加保存：道東限定ミートスパカツ\n",
      "追加保存：きのこと鶏肉のクリームパスタ\n",
      "追加保存：つぶつぶ生たらこをのせた和風パスタ\n",
      "追加保存：大盛明太マヨのスパゲティ\n",
      "追加保存：静岡限定和風冷製パスタツナと大根おろし\n",
      "追加保存：冷製パスタ海老のトマトクリーム\n",
      "追加保存：魚介の旨塩大盛パスタ\n",
      "追加保存：チキンステーキのせバター醤油パスタ\n",
      "追加保存：ボンゴレロッソあさりのトマトソース\n",
      "追加保存：よくばりパスタミートボールのせトマトソース\n",
      "追加保存：大盛明太マヨのスパゲティ\n",
      "追加保存：冷製トマトソーストマトと蒸し鶏のパスタ\n",
      "追加保存：麺大盛ソーセージのペペロンチーノ\n",
      "追加保存：ツナと大根おろしの和風パスタ\n",
      "追加保存：大盛海老といかの旨塩パスタ\n",
      "追加保存：道東限定塩味スパゲティ\n",
      "追加保存：海老といかの旨塩パスタ\n",
      "追加保存：大盛明太マヨのスパゲティ\n",
      "追加保存：アスパラとベーコンのバター醤油パスタ\n",
      "追加保存：冷製パスタ豚しゃぶごまポン酢\n",
      "追加保存：ボンゴレロッソあさりのトマトソース\n",
      "追加保存：大盛トマトとニンニクのパスタ\n",
      "追加保存：高菜と大盛スパゲティ\n",
      "追加保存：ＺＥＮＢヌードル使用彩り野菜のトマトソース\n",
      "追加保存：スパゲットーニにんにくトマト\n",
      "追加保存：小松菜のバター醤油パスタ\n",
      "追加保存：冷製パスタ生ハムとバジルクリーム\n",
      "追加保存：冷製パスタトマトと生ハムとチーズ\n",
      "追加保存：コクと旨味が広がるボンゴレスープパスタ\n",
      "追加保存：肉の旨味あふれるミートソース\n",
      "追加保存：大盛明太マヨのスパゲティ\n",
      "追加保存：肉の旨味あふれるミートソース\n",
      "追加保存：高崎パスタベスビオ魚介の辛口トマトスープパスタ\n",
      "追加保存：ソースたっぷり大盛カルボナーラ\n",
      "追加保存：タルタル盛り鶏唐揚げペペロンチーノ\n",
      "追加保存：明太クリームパスタフェットチーネ使用\n",
      "追加保存：トマトソースとチーズのパスタ\n",
      "追加保存：３種きのこのバター醤油パスタ\n",
      "追加保存：明太マヨのスパゲティ博多辛子明太子使用\n",
      "追加保存：ごまぽん酢で食べる夏の豚しゃぶ冷製パスタ\n",
      "追加保存：太麺仕立ての昔なつかしナポリタン\n",
      "追加保存：３種チーズとろけるトマトソースパスタ\n",
      "追加保存：スパゲットーニナポカレー\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/pasta/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94988444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/gratin/\n",
      "取得中: https://www.sej.co.jp/products/a/gratin/2/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/gratin/1/l15/\n",
      "商品数: 17\n",
      "追加保存：チーズ香る濃厚ソースの海老ドリア\n",
      "追加保存：４種チーズのマカロニグラタン\n",
      "追加保存：３種チーズのミートソースドリア\n",
      "追加保存：とろーりチーズのクリームソースドリア\n",
      "追加保存：３種チーズのミートソースドリア\n",
      "追加保存：明太クリームドリア\n",
      "追加保存：かぼちゃのクリームグラタン\n",
      "追加保存：濃厚ホワイトソースの海老グラタン\n",
      "追加保存：３種チーズのマカロニグラタン\n",
      "追加保存：４種きのことベーコンのグラタン\n",
      "追加保存：濃厚なめらかソースの海老グラタン\n",
      "追加保存：スパイス香る野菜とチキンの焼きカレードリア\n",
      "追加保存：トマトソースのオムライスドリア\n",
      "追加保存：チーズ香る濃厚ソースの海老ドリア\n",
      "追加保存：とろーり濃厚チーズの海老グラタン\n",
      "追加保存：３種チーズのミートソースドリア\n",
      "追加保存：濃厚なめらかソースの海老グラタン\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/gratin/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6a71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/dailydish/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/2/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/3/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/4/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/5/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/6/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/7/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/8/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/9/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/10/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/1/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/11/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/12/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/13/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/14/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/dailydish/15/l15/\n",
      "商品数: 218\n",
      "追加保存：旨辛やみつき豚キムチ\n",
      "追加保存：青森県産地養豚のコクうま豚汁\n",
      "追加保存：７プレミアムおでん（カップ）\n",
      "追加保存：四川風麻婆豆腐\n",
      "追加保存：合わせ味噌仕立ての具だくさん豚汁\n",
      "追加保存：炭火焼鳥しお（七味付き）\n",
      "追加保存：ラクサ海老とスパイスのココナッツスープ\n",
      "追加保存：７プレミアム濃厚スープのもつ煮込み\n",
      "追加保存：じゃがバター＆ソーセージ\n",
      "追加保存：７プレミアムゴールド金の紅鮭の塩焼\n",
      "追加保存：油揚げの炭火焼（葱・大根おろし添え）\n",
      "追加保存：炭火焼き鳥皮３本・もも２本入り\n",
      "追加保存：あごだし仕立ての茶碗蒸し\n",
      "追加保存：７プレミアムきんぴらごぼう\n",
      "追加保存：揚げだし豆腐\n",
      "追加保存：７プレミアムおでん（カップ）\n",
      "追加保存：７プレミアム半熟塩味のゆでたまご２個入り\n",
      "追加保存：７プレミアム味付き半熟とろっとゆでたまご\n",
      "追加保存：お肉のうまみぶた汁九州味噌\n",
      "追加保存：７プレミアムザーサイ炒め\n",
      "追加保存：揚げだし豆腐\n",
      "追加保存：７プレミアム北海道男爵いもの肉じゃが\n",
      "追加保存：７プレミアムごま昆布２Ｐ\n",
      "追加保存：１／２日分の野菜が摂れる野菜炒め\n",
      "追加保存：明太子とポテトのチーズ焼き\n",
      "追加保存：７プレミアム味付メンマ７０ｇ\n",
      "追加保存：７プレミアム茶碗蒸し\n",
      "追加保存：７プレミアム厚焼き玉子\n",
      "追加保存：７プレミアムゴールド金の豚角煮\n",
      "追加保存：７プレミアムゴールド金のハンバーグ\n",
      "追加保存：７プレミアムゴールド金の海老チリソース\n",
      "追加保存：７プレミアムだし巻き玉子\n",
      "追加保存：豚肉ときくらげのふんわり中華玉子炒め\n",
      "追加保存：炭火焼塩ホルモン\n",
      "追加保存：香味醤油仕立てのレバニラ\n",
      "追加保存：鉄板仕立てとん平焼き\n",
      "追加保存：やみつき青菜炒め\n",
      "追加保存：７プレミアム本場韓国産キムチ\n",
      "追加保存：７プレミアム紅しょうが\n",
      "追加保存：じゃがバター＆ソーセージ\n",
      "追加保存：７Ｐ焼きからふとししゃも\n",
      "追加保存：炭火焼鳥しお（別添わさび）\n",
      "追加保存：７Ｐ直火焼和風おろしハンバーグ\n",
      "追加保存：７Ｐ鉄板焼きハンバーグガーリックソース\n",
      "追加保存：冷やっこセット（醤油付）\n",
      "追加保存：温泉玉子\n",
      "追加保存：玉子と木耳の中華炒め青森県産地養豚使用\n",
      "追加保存：７プレミアム味付き半熟ゆでたまご１個入\n",
      "追加保存：炭火焼鳥しお（別添わさび）\n",
      "追加保存：明太マヨネーズと甘辛タレのちくわ天\n",
      "追加保存：福岡県産小松菜の玉子炒め\n",
      "追加保存：ぽん酢で食べる豚しゃぶもやし道産豚肉使用\n",
      "追加保存：７Ｐ鉄板焼きハンバーグチーズインデミソース\n",
      "追加保存：７プレミアムゴールド金のビーフシチュー\n",
      "追加保存：きのこのクリームスープ\n",
      "追加保存：７プレミアム赤だし仕立て豚汁\n",
      "追加保存：７プレミアム金時豆\n",
      "追加保存：だし巻き玉子と炭火焼鳥九州産ふりそで肉使用\n",
      "追加保存：おつまみ厚揚げ\n",
      "追加保存：揚げだし豆腐\n",
      "追加保存：７Ｐとろーり４種のチーズハンバーグ\n",
      "追加保存：７プレミアムあじの塩焼\n",
      "追加保存：ねぎ盛り甘口醤油だれの焼もつ\n",
      "追加保存：ねぎ盛り甘口醤油だれの焼もつ\n",
      "追加保存：ＢＢＱチーズソースポテト＆唐揚げ\n",
      "追加保存：７プレミアムトッピング用温泉たまご\n",
      "追加保存：１／２日分の野菜ポン酢で食べる水餃子\n",
      "追加保存：野菜とベーコンのコンソメスープ\n",
      "追加保存：７プレミアムうの花\n",
      "追加保存：おろしチキンかつだし醤油仕立て\n",
      "追加保存：醤油ワンタンスープ\n",
      "追加保存：ごろっと具材のポトフ\n",
      "追加保存：７プレミアム銀鮭の塩焼\n",
      "追加保存：７プレミアム味付き半熟ゆでたまご２個入\n",
      "追加保存：炭火焼ミックスホルモン塩\n",
      "追加保存：宇都宮餃子会監修野菜焼餃子\n",
      "追加保存：ポン酢で食べる豚もやし\n",
      "追加保存：おつまみラー油の水餃子新潟県産米粉入り\n",
      "追加保存：キムチチゲ\n",
      "追加保存：７プレミアム福神漬\n",
      "追加保存：７プレミアムだし巻き玉子\n",
      "追加保存：おつまみ冷奴豚しゃぶごまぽん酢\n",
      "追加保存：７プレミアムゴールド金の紅鮭の塩焼\n",
      "追加保存：冷やっこセット（醤油付）\n",
      "追加保存：たんぱく質が摂れる炭火焼き鶏はらみ\n",
      "追加保存：丸かじりチキボン\n",
      "追加保存：チキンステーキ（山わさび入りソース使用）\n",
      "追加保存：焼き餃子６個入り\n",
      "追加保存：ねぎ盛り焼豚\n",
      "追加保存：コク旨肉味噌のピリ辛麻婆茄子\n",
      "追加保存：たんぱく質が摂れるレバニラ炒め\n",
      "追加保存：７Ｐ鉄板焼きハンバーグ和風おろしソース\n",
      "追加保存：７プレミアム赤魚の煮付\n",
      "追加保存：７プレミアムゴールド金のビーフカレー\n",
      "追加保存：肉野菜炒め青森県産地養豚使用\n",
      "追加保存：レバニラ\n",
      "追加保存：ソーセージとポテトのマヨネーズ焼き\n",
      "追加保存：広島県産小松菜使用甘辛玉子炒め\n",
      "追加保存：７プレミアムほっけの塩焼\n",
      "追加保存：７プレミアムさばの塩焼\n",
      "追加保存：７プレミアム味付き半熟とろっとゆでたまご２個\n",
      "追加保存：鶏の炭火焼（柚子こしょう付）\n",
      "追加保存：北海道産じゃがいものチーズ焼き\n",
      "追加保存：じゃがバター\n",
      "追加保存：７プレミアム野菜豆\n",
      "追加保存：たんぱく質が摂れる道産豚レバー\n",
      "追加保存：旨辛スンドゥブ\n",
      "追加保存：７プレミアムさばの塩焼\n",
      "追加保存：７プレミアム半熟煮たまご２個入\n",
      "追加保存：和風玉葱ソースのハンバーグ\n",
      "追加保存：トムヤムクン春雨入り\n",
      "追加保存：７プレミアムトマトクリームスープ\n",
      "追加保存：焼き餃子\n",
      "追加保存：ひとくち焼きいもバター\n",
      "追加保存：シビれる辛さＷ麻婆麻婆豆腐・麻婆春雨\n",
      "追加保存：期間限定にんにく＆黒胡椒餃子\n",
      "追加保存：７プレミアムほっけの塩焼\n",
      "追加保存：レンジで仕上げる豚キムチ\n",
      "追加保存：ジャークチキン\n",
      "追加保存：ガーリックトマト仕立てポテト＆ソーセージ\n",
      "追加保存：７プレミアム栗と６種具材入り濃厚な茶わんむし１６０ｇ\n",
      "追加保存：７プレミアム茶碗蒸し\n",
      "追加保存：鶏の炭火焼（柚子こしょう付）\n",
      "追加保存：７プレミアムあじの塩焼\n",
      "追加保存：７プレミアム大根おろし\n",
      "追加保存：香ばし炭火焼き豚ロース\n",
      "追加保存：丸かじりチキボンとポテトのチーズ焼き\n",
      "追加保存：７プレミアム鶏中華かゆ\n",
      "追加保存：７プレミアム銀鮭の塩焼切落し\n",
      "追加保存：７プレミアム国産豚もつ煮込み\n",
      "追加保存：たんぱく質が摂れる砂肝の黒胡椒焼き\n",
      "追加保存：７プレミアムきんぴらごぼう\n",
      "追加保存：７プレミアムゴールド金の銀だらの西京焼\n",
      "追加保存：７プレミアムじゃがいものポタージュ\n",
      "追加保存：とろとろ食感の山芋鉄板\n",
      "追加保存：明太子とポテトのチーズ焼き\n",
      "追加保存：静岡県産わさびで食べる炭火焼鳥\n",
      "追加保存：炭火焼ねぎ塩ホルモン\n",
      "追加保存：７プレミアムさばの味噌煮\n",
      "追加保存：和風だしのふわふわ玉子スープ\n",
      "追加保存：旨み味わうやわらか肉焼売\n",
      "追加保存：炭火焼鳥しお（別添わさび）\n",
      "追加保存：明太子とポテトのチーズ焼き\n",
      "追加保存：７プレミアムにんにく醤油味\n",
      "追加保存：豚汁西京味噌使用\n",
      "追加保存：スパイシーチリトマトポテト＆ソーセージ\n",
      "追加保存：７プレミアムタン塩焼き\n",
      "追加保存：ケイジャンチキン\n",
      "追加保存：鉄板で焼いたにらチヂミ\n",
      "追加保存：おつまみ鶏カラ＆ソーセージ\n",
      "追加保存：７プレミアム黒豆\n",
      "追加保存：栃尾の油揚げねぎ・生姜\n",
      "追加保存：７プレミアムおでん\n",
      "追加保存：ねぎ盛り甘口醤油だれの焼もつ\n",
      "追加保存：こんにゃく麺で食べる辛麺スープ\n",
      "追加保存：たんぱく質が摂れるレバニラ\n",
      "追加保存：７プレミアム銀鮭の塩焼\n",
      "追加保存：７プレミアムゴールド骨付き肉３本入金のバターチキンカレー\n",
      "追加保存：７プレミアム旨辛牛すじ煮込み\n",
      "追加保存：７プレミアム焼きからふとししゃも\n",
      "追加保存：チキンステーキガーリックぺッパー\n",
      "追加保存：たんぱく質が摂れる砂肝の黒胡椒焼き\n",
      "追加保存：和風玉葱ソースのハンバーグ\n",
      "追加保存：揚げだし豆腐\n",
      "追加保存：１／３日分の野菜もっちり餃子スープ\n",
      "追加保存：野菜とソーセージの具だくさんポトフ\n",
      "追加保存：ゴーヤーチャンプルー\n",
      "追加保存：１０品目具材の豚汁（加賀みそ使用）\n",
      "追加保存：７プレミアム甘酢肉だんご\n",
      "追加保存：冷やっこセット（醤油たれ付）\n",
      "追加保存：たんぱく質が摂れる国産砂ずりの黒胡椒焼き\n",
      "追加保存：じゃがバター＆ソーセージ\n",
      "追加保存：ふわっと鶏だし香る茶碗蒸し\n",
      "追加保存：７プレミアムらっきょう\n",
      "追加保存：炭火焼き道産ホルモン（ハツ・レバー入り）\n",
      "追加保存：ソーセージとポテトのマヨネーズ焼き\n",
      "追加保存：こんにゃく麺で食べるトマト辛麺スープ\n",
      "追加保存：鶏むね肉の大葉焼鳥梅だれ\n",
      "追加保存：黒胡椒香るたんぱく質が摂れる参鶏湯\n",
      "追加保存：７プレミアムさばの味噌煮\n",
      "追加保存：焼き餃子\n",
      "追加保存：国産大豆使用冷奴セット\n",
      "追加保存：三陸産わかめたっぷりうま塩わかめスープ\n",
      "追加保存：炭火焼き鳥もも３本・皮２本入り\n",
      "追加保存：旨辛スンドゥブ\n",
      "追加保存：７プレミアムミートボール\n",
      "追加保存：７プレミアムひじき煮\n",
      "追加保存：７プレミアム北海道男爵いもと牛肉の肉じゃが\n",
      "追加保存：ひとくち焼ちくわの磯辺揚げチーズ\n",
      "追加保存：野菜のうまみ広島の味だんご汁\n",
      "追加保存：ふわふわ山芋キャベツ焼き\n",
      "追加保存：越後味噌仕立ての豚汁新潟県産豚肉入り\n",
      "追加保存：７プレミアム半熟塩味のゆでたまご１個入り\n",
      "追加保存：ぽてとチーズもっち\n",
      "追加保存：千とせ本店監修肉吸い\n",
      "追加保存：７プレミアム豚汁\n",
      "追加保存：７プレミアムとうもろこしのポタージュ\n",
      "追加保存：７プレミアム味付きとろろ\n",
      "追加保存：鉄板焼きニラチヂミ\n",
      "追加保存：砂肝にんにくブロッコリー\n",
      "追加保存：枝豆沖縄の塩シママース使用\n",
      "追加保存：生姜香る中身汁\n",
      "追加保存：炭火焼ミックスホルモン（塩）\n",
      "追加保存：７プレミアムさばの塩焼\n",
      "追加保存：７プレミアム切干し大根煮\n",
      "追加保存：７プレミアム穂先メンマ\n",
      "追加保存：丸かじりチキボンとポテトのチーズ焼き\n",
      "追加保存：揚げだし豆腐\n",
      "追加保存：炭火焼ミックスホルモン（塩）\n",
      "追加保存：具だくさん赤だし豚汁\n",
      "追加保存：７プレミアム紀州南高梅うす塩味\n",
      "追加保存：濃い豆腐の冷やっこ（生姜入だし醤油付）\n",
      "追加保存：おつまみ油揚げ（ねぎ・納豆）\n",
      "追加保存：キムチ鍋\n",
      "追加保存：甘エビの唐揚げ\n",
      "追加保存：野菜タンメンスープ\n",
      "追加保存：７プレミアム赤魚の煮付\n",
      "追加保存：７プレミアムおでん\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/dailydish/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1595ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/salad/\n",
      "商品数: 24\n",
      "追加保存：７プレミアムコールスロー\n",
      "追加保存：プリプリ海老のパスタサラダ\n",
      "追加保存：たんぱく質が摂れる鶏むね肉サラダ\n",
      "追加保存：７プレミアムマカロニサラダ\n",
      "追加保存：たんぱく質が摂れる鶏むね肉サラダ\n",
      "追加保存：７プレミアムごぼうサラダ\n",
      "追加保存：アスパラベーコン＆タルタルポテト\n",
      "追加保存：７プレミアムごぼうサラダ\n",
      "追加保存：７プレミアムコールスロー\n",
      "追加保存：７プレミアムたまごサラダ\n",
      "追加保存：いわしのマリネオリーブオイル仕立て\n",
      "追加保存：７プレミアム千切りキャベツ\n",
      "追加保存：比叡ゆばとブロッコリーのおかか和え\n",
      "追加保存：大葉と海老の明太子クリームパスタサラダ\n",
      "追加保存：７プレミアムレタスサラダ\n",
      "追加保存：７プレミアムマカロニサラダ\n",
      "追加保存：７プレミアム大根サラダ\n",
      "追加保存：７プレミアム千切りキャベツ\n",
      "追加保存：信州味噌使用肉味噌ラーメンサラダ\n",
      "追加保存：７プレミアムポテトサラダ\n",
      "追加保存：大山どりのごまごまチキン\n",
      "追加保存：但馬の味どりのシャキシャキ野菜サラダ\n",
      "追加保存：ピリッ！とからっキュウ\n",
      "追加保存：ポン酢で食べる砂ずりと玉ねぎスライス\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/salad/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c545a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/sweets/\n",
      "商品数: 12\n",
      "追加保存：ひとくち包みみたらし\n",
      "追加保存：近江ほうじ茶のチーズケーキ\n",
      "追加保存：阿蘇小国ジャージー牛乳使用ミルクムースシュー\n",
      "追加保存：ずっしり塩大福\n",
      "追加保存：出雲ぜんざい学会監修ひんやり出雲ぜんざい\n",
      "追加保存：バター香るフィナンシェ\n",
      "追加保存：ずっしり草もち\n",
      "追加保存：白バラ牛乳使用ミルクプリンケ―キ\n",
      "追加保存：ずっしり豆大福\n",
      "追加保存：たっぷりホイップのダブルシュー近江ほうじ茶\n",
      "追加保存：ずんだが主役のお団子\n",
      "追加保存：くちどけクリーム＆シフォンケーキ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/sweets/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51545ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/hotsnack/\n",
      "商品数: 12\n",
      "追加保存：【盛盛】若鶏のからあげ（むね）２０個\n",
      "追加保存：ＢＩＧポークフランク\n",
      "追加保存：炭火焼き鳥（塩）\n",
      "追加保存：若鶏のからあげ（もも）５個入り\n",
      "追加保存：ジューシー粗挽きソーセージ\n",
      "追加保存：【盛盛】若鶏のからあげ（もも）２０個\n",
      "追加保存：スパイスチキンレッド\n",
      "追加保存：若鶏のからあげ（むね）５個入り\n",
      "追加保存：ザクチキ（背徳のマシマシ）\n",
      "追加保存：北海道産じゃがいもの牛肉コロッケ\n",
      "追加保存：スパイスチキン\n",
      "追加保存：炭火焼き鳥（タレ）\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/hotsnack/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "580ed2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/oden/\n",
      "取得中: https://www.sej.co.jp/products/a/oden/2/l15/\n",
      "取得中: https://www.sej.co.jp/products/a/oden/1/l15/\n",
      "商品数: 16\n",
      "追加保存：おでんもっちりちくわぶ\n",
      "追加保存：おでん味しみ大根\n",
      "追加保存：おでん味しみこんにゃく\n",
      "追加保存：おでん焼豆腐\n",
      "追加保存：おでん味しみ白こんにゃく\n",
      "追加保存：おでん味しみたまご\n",
      "追加保存：おでん木綿厚揚げ\n",
      "追加保存：おでんなんこつ入り鶏つくね串\n",
      "追加保存：おでん味しみ白滝\n",
      "追加保存：おでんウインナー巻\n",
      "追加保存：おでん牛すじ串\n",
      "追加保存：おでんふんわりはんぺん\n",
      "追加保存：おでんお餅の巾着\n",
      "追加保存：おでん味しみ糸こん\n",
      "追加保存：おでん５種野菜のさつま揚げ\n",
      "追加保存：おでん味しみ大根\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/oden/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1867e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/ice_cream/\n",
      "商品数: 12\n",
      "追加保存：森永ピノダブルショコラ\n",
      "追加保存：ハーゲンダッツアソートボックス世界のデザートセレクション\n",
      "追加保存：ロッテクーリッシュ濃密みかん\n",
      "追加保存：７プレミアム北海道バニラバーマルチ\n",
      "追加保存：７プレミアムワッフルコーンチョコ＆ミルク\n",
      "追加保存：７プレミアムシュガーコーンマルチ\n",
      "追加保存：フタバ食品ダンディーチョコバニラマルチ\n",
      "追加保存：７Ｐチョコレートバーりんご\n",
      "追加保存：ハーゲンダッツアソートボックスラバーズアソート\n",
      "追加保存：赤城セルフクラッシュクッキークリーム\n",
      "追加保存：７プレミアム４種のフルーツバーマルチ\n",
      "追加保存：７プレミアム生チョコアイス\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/ice_cream/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dfd5f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/chukaman/\n",
      "商品数: 7\n",
      "追加保存：ふんわり×ごろっと肉まん\n",
      "追加保存：もちふわ×とろ～りピザまん\n",
      "追加保存：ふわふわ×とろっと濃厚ごまあんまん\n",
      "追加保存：もちもち×ずっしり大入り豚まん\n",
      "追加保存：もっちり×ジューシー特製豚まん\n",
      "追加保存：じゃがまるくん（ポテト＆ミート）\n",
      "追加保存：もちふわ×つぶつぶつぶあんまん\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/chukaman/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce10613b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/frozen_foods/\n",
      "商品数: 15\n",
      "追加保存：７プレミアム塩味そら豆\n",
      "追加保存：７プレミアム極上炒飯\n",
      "追加保存：７プレミアムカリッと大学いも\n",
      "追加保存：７プレミアム若鶏もも唐揚げ\n",
      "追加保存：純氷３．５ｋｇ\n",
      "追加保存：７プレミアムロックアイス１．１ｋｇ\n",
      "追加保存：７プレミアムアップルマンゴー\n",
      "追加保存：７プレミアム焼おにぎり\n",
      "追加保存：７プレミアムブルーベリー\n",
      "追加保存：７プレミアム炒飯（袋）\n",
      "追加保存：７プレミアムカルビクッパ\n",
      "追加保存：７プレミアム塩ゆで枝豆\n",
      "追加保存：天然水の氷１．１Ｋｇ\n",
      "追加保存：７プレミアムブロッコリー\n",
      "追加保存：７プレミアムひとくち熟成蜜芋\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/frozen_foods/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "510dae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/7premium/\n",
      "商品数: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/7premium/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c98a9f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/a/sevencafe/\n",
      "商品数: 8\n",
      "追加保存：７カフェアイスカフェラテＬ\n",
      "追加保存：セブンカフェホットコーヒーＬ\n",
      "追加保存：セブンカフェアイスコーヒーＬ\n",
      "追加保存：セブンカフェアイスコーヒーＲ\n",
      "追加保存：７カフェアイスカフェラテＲ\n",
      "追加保存：７カフェホットカフェラテＲ\n",
      "追加保存：７カフェホットカフェラテＬ\n",
      "追加保存：セブンカフェホットコーヒーＲ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/sevencafe/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7625013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得中: https://www.sej.co.jp/products/sevencafebakery/\n",
      "商品数: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for a in soup.select(\"div.list_inner a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャーから次ページ候補を収集\n",
    "        pager = soup.select(\"div.pager a\")\n",
    "        for a in pager:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/sevencafebakery/\"\n",
    "    product_urls = scrape_category_all_pages(category_url)\n",
    "\n",
    "    print(f\"商品数: {len(product_urls)}\")\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418c4d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カテゴリ数: 40\n",
      "\n",
      "--- カテゴリ開始: おにぎり ---\n",
      "取得中: https://www.sej.co.jp/products/a/onigiri/\n",
      "商品URL数: 0\n",
      "\n",
      "--- カテゴリ開始: お寿司 ---\n",
      "取得中: https://www.sej.co.jp/products/a/sushi/\n",
      "商品URL数: 0\n",
      "\n",
      "--- カテゴリ開始: お弁当 ---\n",
      "取得中: https://www.sej.co.jp/products/a/bento/\n",
      "商品URL数: 0\n",
      "\n",
      "--- カテゴリ開始: サンドイッチ・ ロールパン ---\n",
      "取得中: https://www.sej.co.jp/products/a/sandwich/\n",
      "商品URL数: 0\n",
      "\n",
      "--- カテゴリ開始: パン ---\n",
      "取得中: https://www.sej.co.jp/products/a/bread/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category_name, category_url \u001b[38;5;129;01min\u001b[39;00m categories:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- カテゴリ開始: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m     product_urls \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_category_all_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m商品URL数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(product_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m product_urls:\n",
      "Cell \u001b[0;32mIn[4], line 173\u001b[0m, in \u001b[0;36mscrape_category_all_pages\u001b[0;34m(base_url)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m next_url \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_pages:\n\u001b[1;32m    171\u001b[0m                 next_pages\u001b[38;5;241m.\u001b[39mappend(next_url)\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ページ間の待機\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(all_links)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_products.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url, category_name):\n",
    "    response = requests.get(url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name = \"N/A\"\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    if product_name_div:\n",
    "        h1 = product_name_div.find('h1')\n",
    "        if h1:\n",
    "            product_name = h1.get_text(strip=True).replace(\"\\u3000\", \"\")\n",
    "\n",
    "    # 価格\n",
    "    price = \"N/A\"\n",
    "    price_div = soup.find('div', class_='item_price')\n",
    "    if price_div:\n",
    "        price_text = price_div.get_text(strip=True)\n",
    "        match = re.search(r'(\\d+円（税込[\\d\\.]+円）)', price_text)\n",
    "        if match:\n",
    "            price = match.group(1)\n",
    "        else:\n",
    "            price = price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"N/A\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img = product_wrap_div.find('img')\n",
    "        if img and img.has_attr(\"src\"):\n",
    "            picture_url = img[\"src\"]\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_text)\n",
    "\n",
    "    return {\n",
    "        \"category\": category_name,\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避付き）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    fieldnames = [\n",
    "        \"category\", \"product_name\", \"price\", \"picture\", \"url\",\n",
    "        \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"\n",
    "    ]\n",
    "\n",
    "    # 既存URLを読み込み\n",
    "    existing_urls = set()\n",
    "    if file_exists:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_urls.add(row[\"url\"])\n",
    "\n",
    "    if data[\"url\"] in existing_urls:\n",
    "        print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "        return\n",
    "\n",
    "    with open(filename, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "# トップページからカテゴリ一覧を取得\n",
    "def scrape_all_category_urls():\n",
    "    base_url = \"https://www.sej.co.jp/products\"\n",
    "    response = requests.get(base_url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    categories = []\n",
    "    for li in soup.select(\"ul.link-img-list li\"):\n",
    "        a = li.select_one(\"a \")\n",
    "        name = li.select_one(\"figcaption p.ttl\")\n",
    "        if a and name:\n",
    "            url = urljoin(base_url, a[\"href\"])\n",
    "            categories.append((name.get_text(\" \", strip=True), url))\n",
    "\n",
    "    return categories\n",
    "\n",
    "\n",
    "# カテゴリページをクロールして全ページの商品URLを収集\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.encoding = response.apparent_encoding\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # 商品リンク\n",
    "        for a in soup.select(\"div.list_inner\"):\n",
    "            href = a.get(\"a href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャー\n",
    "        for a in soup.select(\"div.pager\"):\n",
    "            href = a.get(\"a href\")\n",
    "            if href:\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", href)\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "        time.sleep(1)  # ページ間の待機\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "\n",
    "# メイン処理\n",
    "if __name__ == \"__main__\":\n",
    "    categories = scrape_all_category_urls()\n",
    "    print(f\"カテゴリ数: {len(categories)}\")\n",
    "\n",
    "    for category_name, category_url in categories:\n",
    "        print(f\"\\n--- カテゴリ開始: {category_name} ---\")\n",
    "        product_urls = scrape_category_all_pages(category_url)\n",
    "        print(f\"商品URL数: {len(product_urls)}\")\n",
    "\n",
    "        for url in product_urls:\n",
    "            try:\n",
    "                product_info = scrape_seven_eleven_product(url, category_name)\n",
    "                save_to_csv(product_info)\n",
    "                time.sleep(1)  # 商品ごとに待機\n",
    "            except Exception as e:\n",
    "                print(f\"エラー：{url} -> {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a888f6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カテゴリ数: 20\n",
      "\n",
      "--- カテゴリ開始: おにぎり ---\n",
      "取得中: https://www.sej.co.jp/products/a/onigiri/\n",
      "商品URL数: 18\n",
      "エラー：https://www.sej.co.jp/products/a/item/047791/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/044395/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047813/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047906/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047794/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047776/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/044425/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047792/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047940/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047801/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047626/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047809/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047683/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047939/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/045492/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047684/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047625/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047786/ -> 'url'\n",
      "\n",
      "--- カテゴリ開始: お寿司 ---\n",
      "取得中: https://www.sej.co.jp/products/a/sushi/\n",
      "商品URL数: 18\n",
      "エラー：https://www.sej.co.jp/products/a/item/044389/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047652/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047807/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/046758/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047767/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047562/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047808/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047611/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047765/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047576/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047737/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047774/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047612/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047066/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047698/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/044448/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047882/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047592/ -> 'url'\n",
      "\n",
      "--- カテゴリ開始: お弁当 ---\n",
      "取得中: https://www.sej.co.jp/products/a/bento/\n",
      "商品URL数: 12\n",
      "エラー：https://www.sej.co.jp/products/a/item/047942/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/044747/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/044393/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/041666/ -> 'url'\n",
      "エラー：https://www.sej.co.jp/products/a/item/047943/ -> 'url'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 180\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m product_urls:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m         product_info \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_seven_eleven_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m         save_to_csv(product_info)\n\u001b[1;32m    182\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m, in \u001b[0;36mscrape_seven_eleven_product\u001b[0;34m(url, category_name)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_seven_eleven_product\u001b[39m(url, category_name):\n\u001b[0;32m---> 43\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# 商品名\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_products.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url, category_name):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込[\\d\\.]+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"N/A\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"category\": category_name,\n",
    "        \"product_name\": product_name or \"N/A\",\n",
    "        \"price\": price or \"N/A\",\n",
    "        \"picture\": picture_url or \"N/A\",\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "# CSVに保存（重複回避付き）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    if not data or \"url\" not in data:\n",
    "        print(f\"スキップ（データ不正）: {data}\")\n",
    "        return\n",
    "\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    fieldnames = [\"category\",\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    # 既存URL読み込み\n",
    "    existing_urls = set()\n",
    "    if file_exists:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_urls.add(row[\"url\"])\n",
    "\n",
    "    if data[\"url\"] in existing_urls:\n",
    "        print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "        return\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "# 指定のdivだけからカテゴリを取得\n",
    "def scrape_specific_category():\n",
    "    base_url = \"https://www.sej.co.jp/products\"\n",
    "    response = requests.get(base_url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    target_div = soup.find(\"div\", id=\"pbBlock3329309\")\n",
    "    categories = []\n",
    "\n",
    "    if target_div:\n",
    "        for li in target_div.select(\"ul.link-img-list li\"):\n",
    "            a = li.select_one(\"a\")\n",
    "            name = li.select_one(\"figcaption p.ttl\")\n",
    "            if a and name and a.has_attr(\"href\"):\n",
    "                url = urljoin(base_url, a[\"href\"])\n",
    "                categories.append((name.get_text(\" \", strip=True), url))\n",
    "    return categories\n",
    "\n",
    "# カテゴリページの全商品URLを取得（全ページ対応）\n",
    "def scrape_category_all_pages(base_url):\n",
    "    all_links = set()\n",
    "    visited_pages = set()\n",
    "    next_pages = [base_url]\n",
    "\n",
    "    while next_pages:\n",
    "        url = next_pages.pop(0)\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        print(f\"取得中: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # 商品リンク収集\n",
    "        for div in soup.select(\"div.list_inner\"):\n",
    "            a = div.find(\"a\")\n",
    "            if a and a.has_attr(\"href\") and \"/products/\" in a[\"href\"]:\n",
    "                full_url = urljoin(\"https://www.sej.co.jp\", a[\"href\"])\n",
    "                all_links.add(full_url)\n",
    "\n",
    "        # ページャー\n",
    "        for a in soup.select(\"div.pager a\"):\n",
    "            if a.has_attr(\"href\"):\n",
    "                next_url = urljoin(\"https://www.sej.co.jp\", a[\"href\"])\n",
    "                if next_url not in visited_pages:\n",
    "                    next_pages.append(next_url)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "# メイン処理\n",
    "if __name__ == \"__main__\":\n",
    "    categories = scrape_specific_category()\n",
    "    print(f\"カテゴリ数: {len(categories)}\")\n",
    "\n",
    "    for category_name, category_url in categories:\n",
    "        print(f\"\\n--- カテゴリ開始: {category_name} ---\")\n",
    "        product_urls = scrape_category_all_pages(category_url)\n",
    "        print(f\"商品URL数: {len(product_urls)}\")\n",
    "\n",
    "        for url in product_urls:\n",
    "            try:\n",
    "                product_info = scrape_seven_eleven_product(url, category_name)\n",
    "                save_to_csv(product_info)\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca69ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== セブンイレブン サラダ商品スクレイピング開始 ===\n",
      "対象ページ数: 1\n",
      "\n",
      "=== ページ 1/1 を処理中 ===\n",
      "URL: https://www.sej.co.jp/products/a/7premium/fresh/1/l100/\n",
      "商品要素を発見: 100件\n",
      "サラダ商品を発見: ７Ｐサラダチキンバー　バジル＆オリーブ\n",
      "サラダ商品を発見: ７Ｐサラダチキンバー　スモークペッパー\n",
      "サラダ商品を発見: ７プレミアム　サラダチキン　スモーク\n",
      "サラダ商品を発見: ７プレミアム　サラダチキン　ハーブ\n",
      "サラダ商品を発見: ７プレミアム　　　　　サラダチキンバー\n",
      "サラダ商品を発見: ７プレミアム　サラダチキン　プレーン\n",
      "サラダ商品を発見: ７プレミアム　ほぐしサラダチキン\n",
      "サラダ商品を 7 件発見\n",
      "  商品 1/7: ７Ｐサラダチキンバー　バジル＆オリーブ\n",
      "  商品 2/7: ７Ｐサラダチキンバー　スモークペッパー\n",
      "  商品 3/7: ７プレミアム　サラダチキン　スモーク\n",
      "  商品 4/7: ７プレミアム　サラダチキン　ハーブ\n",
      "  商品 5/7: ７プレミアム　　　　　サラダチキンバー\n",
      "  商品 6/7: ７プレミアム　サラダチキン　プレーン\n",
      "  商品 7/7: ７プレミアム　ほぐしサラダチキン\n",
      "\n",
      "データを sej_salad_products.csv に保存しました。\n",
      "保存された行数: 7\n",
      "\n",
      "=== 取得データ概要 ===\n",
      "総サラダ商品数: 7\n",
      "\n",
      "各データ項目の取得状況:\n",
      "  product_name: 7/7\n",
      "  price: 7/7\n",
      "  picture: 7/7\n",
      "  calorie: 7/7\n",
      "  protein: 7/7\n",
      "  fat: 6/7\n",
      "  carbohydrate: 7/7\n",
      "  sugar: 7/7\n",
      "  fiber: 7/7\n",
      "  salt: 7/7\n",
      "\n",
      "=== サンプルデータ ===\n",
      "\n",
      "商品 1:\n",
      "  商品名: ７Ｐサラダチキンバー　バジル＆オリーブ\n",
      "  価格: 148円\n",
      "  カロリー: 59kcal\n",
      "  たんぱく質: 12.5g\n",
      "  脂質: 0.9g\n",
      "  炭水化物: 0.0g\n",
      "  糖質: 0.0g\n",
      "  食物繊維: 0.0g\n",
      "  食塩相当量: 0.8g\n",
      "  商品URL: https://www.sej.co.jp/products/a/item/251383/\n",
      "  取得元ページ: https://www.sej.co.jp/products/a/7premium/fresh/1/l100/\n",
      "\n",
      "商品 2:\n",
      "  商品名: ７Ｐサラダチキンバー　スモークペッパー\n",
      "  価格: 148円\n",
      "  カロリー: 63kcal\n",
      "  たんぱく質: 13.6g\n",
      "  脂質: 0.9g\n",
      "  炭水化物: 0.0g\n",
      "  糖質: 0.0g\n",
      "  食物繊維: 0.0g\n",
      "  食塩相当量: 1g\n",
      "  商品URL: https://www.sej.co.jp/products/a/item/250257/\n",
      "  取得元ページ: https://www.sej.co.jp/products/a/7premium/fresh/1/l100/\n",
      "\n",
      "商品 3:\n",
      "  商品名: ７プレミアム　サラダチキン　スモーク\n",
      "  価格: 238円\n",
      "  カロリー: 117kcal\n",
      "  たんぱく質: 26.0g\n",
      "  脂質: 1.3g\n",
      "  炭水化物: 0.0g\n",
      "  糖質: 0.0g\n",
      "  食物繊維: 0.0g\n",
      "  食塩相当量: 1.9g\n",
      "  商品URL: https://www.sej.co.jp/products/a/item/250593/\n",
      "  取得元ページ: https://www.sej.co.jp/products/a/7premium/fresh/1/l100/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "\n",
    "class SejSaladScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.sej.co.jp\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "        })\n",
    "    \n",
    "    def get_products_from_page(self, page_url):\n",
    "        \"\"\"指定されたページから商品一覧を取得\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(page_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            products = []\n",
    "            \n",
    "            # 参考コードのHTML構造に合わせたセレクタを使用\n",
    "            # div class=\"list_inner -item-code-XXXXXX\"\n",
    "            product_elements = soup.select('div[class*=\"list_inner -item-code-\"]')\n",
    "            \n",
    "            if not product_elements:\n",
    "                print(f\"警告: 商品要素が見つかりませんでした。セレクタ '.list_inner -item-code-*' を確認してください。\")\n",
    "                return []\n",
    "\n",
    "            print(f\"商品要素を発見: {len(product_elements)}件\")\n",
    "            \n",
    "            for element in product_elements:\n",
    "                try:\n",
    "                    # 商品名を取得\n",
    "                    # <div class=\"item_ttl\"><p><a href=\"...\">７Ｐサラダチキンバー　バジル＆オリーブ</a></p></div>\n",
    "                    product_name_element = element.select_one('.item_ttl p a')\n",
    "                    product_name = product_name_element.get_text(strip=True) if product_name_element else None\n",
    "                    \n",
    "                    # 「サラダ」が含まれる商品のみを対象にする\n",
    "                    if product_name and 'サラダ' in product_name:\n",
    "                        # 商品詳細ページのURLを取得\n",
    "                        # <figure><a href=\"/products/a/item/251383/\"><img ...></a></figure>\n",
    "                        product_url_element = element.select_one('figure a')\n",
    "                        product_url = urljoin(self.base_url, product_url_element['href']) if product_url_element and 'href' in product_url_element.attrs else None\n",
    "                        \n",
    "                        # 価格を取得\n",
    "                        # <div class=\"item_price\"><p>148円（税込159.84円）</p></div>\n",
    "                        price = None\n",
    "                        price_element = element.select_one('.item_price p')\n",
    "                        if price_element:\n",
    "                            price_text = price_element.get_text(strip=True)\n",
    "                            price_match = re.search(r'(\\d+(?:,\\d+)?)', price_text.replace(',', ''))\n",
    "                            if price_match:\n",
    "                                price = price_match.group(1)\n",
    "                        \n",
    "                        # 画像を取得\n",
    "                        # <img data-original=\"...\" alt=\"\" class=\"lazy\" src=\"...\">\n",
    "                        picture = None\n",
    "                        img_element = element.select_one('figure img')\n",
    "                        if img_element:\n",
    "                            # data-original属性があればそちらを優先、なければsrc\n",
    "                            picture = urljoin(self.base_url, img_element.get('data-original', img_element.get('src', '')))\n",
    "                        \n",
    "                        products.append({\n",
    "                            'product_name': product_name,\n",
    "                            'price': price,\n",
    "                            'picture': picture,\n",
    "                            'url': product_url,\n",
    "                            'source_page': page_url\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"サラダ商品を発見: {product_name}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"商品情報取得エラー (要素: {element.prettify()[:200]}...): {e}\") # エラー発生箇所のHTMLを一部表示\n",
    "                    continue\n",
    "            \n",
    "            return products\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ページ取得エラー ({page_url}): {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_nutrition_info(self, soup):\n",
    "        \"\"\"栄養成分情報を抽出\"\"\"\n",
    "        nutrition_data = {\n",
    "            'calorie': None,\n",
    "            'protein': None,\n",
    "            'fat': None,\n",
    "            'carbohydrate': None,\n",
    "            'sugar': None,\n",
    "            'fiber': None,\n",
    "            'salt': None\n",
    "        }\n",
    "        \n",
    "        # 栄養成分表を探す\n",
    "        nutrition_sections = []\n",
    "        \n",
    "        # テーブル形式\n",
    "        tables = soup.find_all('table')\n",
    "        for table in tables:\n",
    "            table_text = table.get_text()\n",
    "            if any(keyword in table_text for keyword in ['エネルギー', 'カロリー', 'たんぱく質', '栄養成分', '栄養']):\n",
    "                nutrition_sections.append(table)\n",
    "        \n",
    "        # div要素\n",
    "        nutrition_divs = soup.find_all('div', string=re.compile(r'栄養成分|エネルギー|カロリー|栄養'))\n",
    "        for div in nutrition_divs:\n",
    "            parent = div.find_parent()\n",
    "            if parent:\n",
    "                nutrition_sections.append(parent)\n",
    "        \n",
    "        # クラス名で探す\n",
    "        # セブンイレブンの商品詳細ページでよく見られるセレクタを追加\n",
    "        nutrition_classes = [\n",
    "            '.item-nutritional-info', # 例: <div class=\"item-nutritional-info\">\n",
    "            '.product-info-table',    # 例: <table class=\"product-info-table\">\n",
    "            '.nutrition-facts',       # 一般的なクラス名\n",
    "            '.nutrition',\n",
    "            '.nutritional-info',\n",
    "            '.product-info',\n",
    "            '.detail-info'\n",
    "        ]\n",
    "        \n",
    "        for class_name in nutrition_classes:\n",
    "            elements = soup.select(class_name) # selectを使用\n",
    "            nutrition_sections.extend(elements)\n",
    "\n",
    "        # 重複する要素を排除\n",
    "        nutrition_sections = list(set(nutrition_sections))\n",
    "        \n",
    "        # 栄養成分を抽出\n",
    "        for section in nutrition_sections:\n",
    "            if section:\n",
    "                text = section.get_text(separator=' ', strip=True) # 改行や複数の空白を整理\n",
    "                \n",
    "                patterns = {\n",
    "                    'calorie': [\n",
    "                        r'エネルギー[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*(?:kcal|キロカロリー)?',\n",
    "                        r'カロリー[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*(?:kcal)?',\n",
    "                        r'(\\d+(?:\\.\\d+)?)\\s*kcal'\n",
    "                    ],\n",
    "                    'protein': [\n",
    "                        r'たんぱく質[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?',\n",
    "                        r'蛋白質[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?',\n",
    "                        r'タンパク質[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?'\n",
    "                    ],\n",
    "                    'fat': [\n",
    "                        r'脂質[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?',\n",
    "                        r'脂肪[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?'\n",
    "                    ],\n",
    "                    'carbohydrate': [\n",
    "                        r'炭水化物[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?'\n",
    "                    ],\n",
    "                    'sugar': [\n",
    "                        r'糖質[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?',\n",
    "                        r'糖分[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?'\n",
    "                    ],\n",
    "                    'fiber': [\n",
    "                        r'食物繊維[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?',\n",
    "                        r'繊維[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?'\n",
    "                    ],\n",
    "                    'salt': [\n",
    "                        r'食塩相当量[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?',\n",
    "                        r'塩分[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?',\n",
    "                        r'ナトリウム[：:\\s]*(\\d+(?:\\.\\d+)?)mg?', # ナトリウムの場合はmgも考慮\n",
    "                        r'食塩[：:\\s]*(\\d+(?:\\.\\d+)?)\\s*g?'\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                for nutrient, regex_list in patterns.items():\n",
    "                    if nutrition_data[nutrient] is None:\n",
    "                        for regex in regex_list:\n",
    "                            match = re.search(regex, text, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                value = match.group(1)\n",
    "                                # ナトリウムを食塩相当量に変換する場合のロジックを追加（任意）\n",
    "                                if nutrient == 'salt' and 'ナトリウム' in regex and 'mg' in text:\n",
    "                                    try:\n",
    "                                        # ナトリウム(mg) * 2.54 / 1000 = 食塩相当量(g)\n",
    "                                        # ただし、すでに食塩相当量として表示されている場合もあるので、優先順位は調整\n",
    "                                        sodium_mg = float(value)\n",
    "                                        value = str(round(sodium_mg * 2.54 / 1000, 2))\n",
    "                                    except ValueError:\n",
    "                                        pass # 変換失敗時はそのまま\n",
    "                                nutrition_data[nutrient] = value\n",
    "                                break\n",
    "        \n",
    "        return nutrition_data\n",
    "    \n",
    "    def scrape_product_detail(self, product_info):\n",
    "        \"\"\"商品詳細ページから栄養成分情報を取得\"\"\"\n",
    "        try:\n",
    "            if not product_info.get('url'):\n",
    "                print(f\"  詳細URL無し: {product_info.get('product_name', 'Unknown')}\")\n",
    "                return {**product_info, **{\n",
    "                    'calorie': None, 'protein': None, 'fat': None,\n",
    "                    'carbohydrate': None, 'sugar': None, 'fiber': None, 'salt': None\n",
    "                }}\n",
    "            \n",
    "            response = self.session.get(product_info['url'])\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 栄養成分情報を取得\n",
    "            nutrition_info = self.extract_nutrition_info(soup)\n",
    "            \n",
    "            # 栄養成分が取得できなかった場合に、別のセレクタを試すなど、さらに頑健性を高めることも可能\n",
    "            \n",
    "            return {**product_info, **nutrition_info}\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            print(f\"  HTTPエラー ({product_info.get('product_name', 'Unknown')}, URL: {product_info.get('url')}): {http_err}\")\n",
    "            return {**product_info, **{\n",
    "                'calorie': None, 'protein': None, 'fat': None,\n",
    "                'carbohydrate': None, 'sugar': None, 'fiber': None, 'salt': None\n",
    "            }}\n",
    "        except Exception as e:\n",
    "            print(f\"  商品詳細取得エラー ({product_info.get('product_name', 'Unknown')}, URL: {product_info.get('url')}): {e}\")\n",
    "            return {**product_info, **{\n",
    "                'calorie': None, 'protein': None, 'fat': None,\n",
    "                'carbohydrate': None, 'sugar': None, 'fiber': None, 'salt': None\n",
    "            }}\n",
    "    \n",
    "    def scrape_multiple_pages(self, page_urls, delay=2):\n",
    "        \"\"\"複数ページから全商品の情報を取得\"\"\"\n",
    "        all_products = []\n",
    "        \n",
    "        for i, page_url in enumerate(page_urls, 1):\n",
    "            print(f\"\\n=== ページ {i}/{len(page_urls)} を処理中 ===\")\n",
    "            print(f\"URL: {page_url}\")\n",
    "            \n",
    "            # ページから商品一覧を取得\n",
    "            products_basic_info = self.get_products_from_page(page_url)\n",
    "            print(f\"サラダ商品を {len(products_basic_info)} 件発見\")\n",
    "            \n",
    "            # 各商品の詳細情報を取得\n",
    "            for j, product_info in enumerate(products_basic_info, 1):\n",
    "                print(f\"  商品 {j}/{len(products_basic_info)}: {product_info.get('product_name', 'N/A')}\")\n",
    "                \n",
    "                complete_product_data = self.scrape_product_detail(product_info)\n",
    "                all_products.append(complete_product_data)\n",
    "                \n",
    "                # サーバーへの負荷を軽減\n",
    "                if j < len(products_basic_info):\n",
    "                    time.sleep(delay)\n",
    "            \n",
    "            # ページ間の遅延\n",
    "            if i < len(page_urls):\n",
    "                time.sleep(delay * 2)\n",
    "        \n",
    "        return all_products\n",
    "    \n",
    "    def save_to_csv(self, data, filename='sej_salad_products.csv'):\n",
    "        \"\"\"データをCSVファイルに保存\"\"\"\n",
    "        if not data:\n",
    "            print(\"保存するデータがありません。\")\n",
    "            return False\n",
    "        \n",
    "        columns = [\"product_name\", \"price\", \"picture\", \"calorie\", \"protein\", \"fat\", \n",
    "        \"carbohydrate\", \"sugar\", \"fiber\", \"salt\", \"url\"]\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "                writer.writeheader()\n",
    "                \n",
    "                for row in data:\n",
    "                    filtered_row = {col: row.get(col, '') for col in columns}\n",
    "                    writer.writerow(filtered_row)\n",
    "            \n",
    "            print(f\"\\nデータを {filename} に保存しました。\")\n",
    "            print(f\"保存された行数: {len(data)}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"CSV保存エラー: {e}\")\n",
    "            return False\n",
    "\n",
    "# 使用例\n",
    "def main():\n",
    "    scraper = SejSaladScraper()\n",
    "    \n",
    "    # 指定されたURLリスト\n",
    "    # セブンイレブンの「セブンプレミアム フレッシュ」カテゴリページや、サラダチキンなどがまとまっているページ\n",
    "    page_urls = [\n",
    "        \"https://www.sej.co.jp/products/a/7premium/fresh/1/l100/\" # フレッシュ惣菜など\n",
    "    ]\n",
    "    \n",
    "    print(\"=== セブンイレブン サラダ商品スクレイピング開始 ===\")\n",
    "    print(f\"対象ページ数: {len(page_urls)}\")\n",
    "    \n",
    "    # データを取得\n",
    "    products_data = scraper.scrape_multiple_pages(page_urls, delay=1.5)\n",
    "    \n",
    "    if products_data:\n",
    "        # CSVに保存\n",
    "        success = scraper.save_to_csv(products_data)\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\n=== 取得データ概要 ===\")\n",
    "            print(f\"総サラダ商品数: {len(products_data)}\")\n",
    "            \n",
    "            # データ項目の取得状況\n",
    "            data_status = {}\n",
    "            for key in ['product_name', 'price', 'picture', 'calorie', 'protein', 'fat', 'carbohydrate', 'sugar', 'fiber', 'salt']:\n",
    "                non_null_count = sum(1 for product in products_data if product.get(key) is not None and product.get(key) != '') # Noneと空文字をチェック\n",
    "                data_status[key] = f\"{non_null_count}/{len(products_data)}\"\n",
    "            \n",
    "            print(\"\\n各データ項目の取得状況:\")\n",
    "            for key, status in data_status.items():\n",
    "                print(f\"  {key}: {status}\")\n",
    "            \n",
    "            # サンプルデータを表示\n",
    "            print(\"\\n=== サンプルデータ ===\")\n",
    "            for i, product in enumerate(products_data[:3], 1): # 最初の3件を表示\n",
    "                print(f\"\\n商品 {i}:\")\n",
    "                print(f\"  商品名: {product.get('product_name', 'N/A')}\")\n",
    "                print(f\"  価格: {product.get('price', 'N/A')}円\")\n",
    "                print(f\"  カロリー: {product.get('calorie', 'N/A')}kcal\")\n",
    "                print(f\"  たんぱく質: {product.get('protein', 'N/A')}g\")\n",
    "                print(f\"  脂質: {product.get('fat', 'N/A')}g\")\n",
    "                print(f\"  炭水化物: {product.get('carbohydrate', 'N/A')}g\")\n",
    "                print(f\"  糖質: {product.get('sugar', 'N/A')}g\")\n",
    "                print(f\"  食物繊維: {product.get('fiber', 'N/A')}g\")\n",
    "                print(f\"  食塩相当量: {product.get('salt', 'N/A')}g\")\n",
    "                print(f\"  商品URL: {product.get('url', 'N/A')}\")\n",
    "                print(f\"  取得元ページ: {product.get('source_page', 'N/A')}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"サラダ商品が見つかりませんでした。\")\n",
    "        print(\"指定したURLやページ構造が変更されている可能性があります。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
