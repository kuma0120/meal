{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec35378",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# CSVファイル名\n",
    "filename = \"seven_eleven_product.csv\"\n",
    "\n",
    "# 栄養成分を正規表現で分解する関数\n",
    "def parse_nutrition(text):\n",
    "    nutrition = {\n",
    "        \"calorie\": \"N/A\",\n",
    "        \"protein\": \"N/A\",\n",
    "        \"fat\": \"N/A\",\n",
    "        \"carbohydrate\": \"N/A\",\n",
    "        \"sugar\": \"N/A\",\n",
    "        \"fiber\": \"N/A\",\n",
    "        \"salt\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # 各項目を正規表現で抽出\n",
    "    patterns = {\n",
    "        \"calorie\": r\"熱量[:：]\\s*([\\d\\.]+kcal)\",\n",
    "        \"protein\": r\"たんぱく質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fat\": r\"脂質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"carbohydrate\": r\"炭水化物[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"sugar\": r\"糖質[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"fiber\": r\"食物繊維[:：]\\s*([\\d\\.]+g)\",\n",
    "        \"salt\": r\"食塩相当量[:：]\\s*([\\d\\.]+g)\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            nutrition[key] = match.group(1)\n",
    "\n",
    "    return nutrition\n",
    "\n",
    "\n",
    "# 商品ページから詳細を取得\n",
    "def scrape_seven_eleven_product(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 商品名\n",
    "    product_name_div = soup.find('div', class_='item_ttl')\n",
    "    product_name_tag = product_name_div.find('h1') if product_name_div else None\n",
    "    product_name = product_name_tag.get_text(strip=True).replace(\"\\u3000\", \"\") if product_name_tag else \"N/A\"\n",
    "\n",
    "    # 価格\n",
    "    price_tag = soup.find('div', class_='item_price')\n",
    "    price = \"N/A\"\n",
    "    if price_tag:\n",
    "        price_text = price_tag.text.strip()\n",
    "        price_match = re.search(r'(\\d+円（税込\\d+円）)', price_text)\n",
    "        price = price_match.group(1) if price_match else price_text\n",
    "\n",
    "    # 画像URL\n",
    "    picture_url = \"\"\n",
    "    product_wrap_div = soup.find('div', class_='productWrap')\n",
    "    if product_wrap_div:\n",
    "        img_tag = product_wrap_div.find('img')\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            picture_url = img_tag['src']\n",
    "\n",
    "    # 栄養成分\n",
    "    nutrition_td_text = \"\"\n",
    "    for th in soup.find_all(\"th\"):\n",
    "        if \"栄養成分\" in th.get_text(strip=True):\n",
    "            td = th.find_next(\"td\")\n",
    "            if td:\n",
    "                nutrition_td_text = td.get_text(\" \", strip=True)\n",
    "                break\n",
    "    nutrients = parse_nutrition(nutrition_td_text)\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"picture\": picture_url,\n",
    "        \"url\": url,\n",
    "        **nutrients\n",
    "    }\n",
    "\n",
    "\n",
    "# CSVに保存（重複回避して追記）\n",
    "def save_to_csv(data, filename=filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    fieldnames = [\"product_name\", \"price\", \"picture\", \"url\",\n",
    "                  \"calorie\", \"protein\", \"fat\", \"carbohydrate\", \"sugar\", \"fiber\", \"salt\"]\n",
    "\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # 新規作成時のみヘッダーを書き込む\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # 既存ファイルの重複チェック\n",
    "        if file_exists:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                existing = f.read()\n",
    "            if data[\"url\"] in existing:\n",
    "                print(f\"スキップ（既存）：{data['product_name']}\")\n",
    "                return\n",
    "\n",
    "        writer.writerow(data)\n",
    "        print(f\"追加保存：{data['product_name']}\")\n",
    "\n",
    "\n",
    "# カテゴリページをクロールして商品URLを収集\n",
    "def scrape_category(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    product_links = []\n",
    "    for a in soup.select(\"div.list_inner a\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and \"/products/\" in href:\n",
    "            full_url = \"https://www.sej.co.jp\" + href\n",
    "            product_links.append(full_url)\n",
    "\n",
    "    return list(set(product_links))  # 重複除去\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    category_url = \"https://www.sej.co.jp/products/a/onigiri\"  # おにぎりカテゴリ例\n",
    "    product_urls = scrape_category(category_url)\n",
    "\n",
    "    for url in product_urls:\n",
    "        try:\n",
    "            product_info = scrape_seven_eleven_product(url)\n",
    "            save_to_csv(product_info)\n",
    "        except Exception as e:\n",
    "            print(f\"エラー：{url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfacc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
